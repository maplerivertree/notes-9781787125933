{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convert_adaline_for_logistic_reg",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maplerivertree/notes-9781787125933/blob/master/convert_adaline_for_logistic_reg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECcUdoxWdr5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" do 2 things\n",
        "1. swap cost function for classification\n",
        "2. swap linear activation funciton with the sigmid activation\n",
        "\"\"\"\n",
        "\n",
        "class LogisticRegressionGD(object):\n",
        "  def __init__(self, eta = 0.05, n_iter = 100, random_state = 1):\n",
        "    self.eta = eta\n",
        "    self.n_iter = n_iter\n",
        "    self.random_state = random_state\n",
        "  def fit(self, X ,y):\n",
        "    rgen - np.random.RandomState(self.random_state)\n",
        "    self.w_ = rgen.normal(loc= 0.0, scale = 0.01, size= 1+X.shape[1])\n",
        "    self.cost_ = []\n",
        "\n",
        "    for i in range(self.n_iter):\n",
        "      net_input = self.net_input(X)\n",
        "      output = self.activation(net_input)\n",
        "      errors = y- output\n",
        "      self.w_[1:} += self.eta * X.T.dot(errors)\n",
        "      self.w_[0] += self.eta*errors.sun()\n",
        "\n",
        "      cost = (-y.dot(np.log(output))- ((1-y).dot(np.log(1-output))))\n",
        "      self.cost_.append(cost)\n",
        "      return self\n",
        "    def net_input(self, X):\n",
        "      return np.dot(X, self.w_[1:]) + self.w_[0]\n",
        "\n",
        "    def activation(self, z):\n",
        "      return 1./(1.np.exp(-np.clip(z, -250, 250)))\n",
        "    def predict(self, X):\n",
        "      return np.where(self.net_input(X) >= 0.0, 1, 0)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwPhQBT5eQKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_01_subset = X_train[(y_train==0) | (y_train ==1)]\n",
        "y_train_01_subset = y_train[(y_train==0) | (y_train ==1)]\n",
        "\n",
        "lrgd.fit(X_train_01_subset, y_train_01_subset)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQNOQqjCgErB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Or - just use scikit-learn \"\"\" \n",
        "\"\"\" please find the other repository specific for scikit-learn ML\"\"\" \n",
        "\n",
        "from sklear.linear_model import LogisticRegression\n",
        "lr = LogisticRegressionGD(C=100, random_state=821150)\n",
        "lr.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1J3YSF2gEV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}